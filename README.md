# Snowflake SQL Buildkite Plugin

*NOTE: The intent of this project is to test out an idea and it's not meant for production.*

Runs SQL in Snowflake to allow Buildkite to become a data engineering pipeline orchestration framework like AirFlow.

## Rationale

CONTEXT: I've led data engineering as part of my last two roles. ETL (Extract, Transform, Load) tooling is challenging to develop against and operate.

- Configuring and operating tools is time consuming, usually requiring full time staff.
- AirFlow uses Python specific DSL for creating pipelines - requiring developers to lock into that tool.
- Microsoft Azure Data Factory produces JSON that is hard to peer review using a pull request process and integrate will with Git.
- Connecting with alerting tools like Slack is another item to manage.

BACKGROUND: Data engineering splits into batch data ETL (or ELT) and streaming. Many companies use batch. And the focus for this experiment is batch ETL.

- Examples of tools in this space are https://airflow.apache.org/ and https://azure.microsoft.com/en-us/products/data-factory.
- There are many reasons to use existing tooling for ETL e.g., active community, focus on specifics of ETL, breadth of connectors.

WHY TRY BUILDKITE TOOLS for Data Pipeline Orchestration:

"AirflowÂ® is a batch workflow orchestration platform. The Airflow framework contains operators to connect with many technologies and is easily extensible to connect with a new technology. If your workflows have a clear start and end, and run at regular intervals, they can be programmed as an Airflow DAG." https://airflow.apache.org/docs/apache-airflow/stable/index.html

Builtkite has significant overlaps with AirFlow's capabilities:

|Capability|AirFlow|Buildkite|
|Batch processing|Yes|Yes|
|Schedule jobs / pipelines|Yes|Yes|
|Directed acyclic graph|Yes|Yes|
|Extensible programming model|Yes|Yes|
|Control retries|Yes|Yes|

Buildkite excels in extensibility because it allows developers to use any technology (by working at an OS / Docker level) and stiches interactions together through YAML.

- Buildkite's hybrid architecture gives large customers control over their infrastructure without having to host user interfaces and connectors to alerting systems.
- Existing plugin ecosystem makes it simple to connect with external systems for alerting.

THOUGHT EXPERIMENT: This implementation isn't fully featured, instead designed to test out the idea.

- Value interpolation in SQL would need to be implemented.
- Connecting to Snowflake bindings in API hasn't been built.
- Ability to read results and run jobs based on logic isn't part of the implementation.

## Example

Add the following to your `pipeline.yml`:

```yml
steps:
  - command: ls
    plugins:
      - carlwoodward/snowflake-sql#v1.0.0:
          account: '********'
          jwt: '***************************************************'
          sql_path: 'test.sql'
```

## Configuration

### `account` (Required, string)

Your Snowflake account - i.e., `https://${ACCOUNT}.snowflakecomputing.com/`.

### `jwt` (Required, string)

Your authentication token for working with Snowflake. In the future this will be generated for you by this plugin. For more info on how to generate a Snowflake JWT see https://docs.snowflake.com/en/developer-guide/sql-api/authenticating#label-sql-api-authenticating-key-pair.

### `path` (Required, string)

The path to your SQL file that will be executed on your Snowflake environment.

## Developing

To run the tests:

```shell
docker-compose run --rm tests
```

## Contributing

1. Fork the repo
2. Make the changes
3. Run the tests
4. Commit and push your changes
5. Send a pull request
